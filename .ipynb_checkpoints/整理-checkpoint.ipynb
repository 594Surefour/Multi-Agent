{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB1 MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T00:37:52.155974Z",
     "start_time": "2021-04-28T00:37:52.031244Z"
    }
   },
   "source": [
    "![image.png](./Assort/1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstarct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更具体地定义以下标识：      \n",
    "    S是有限的状态集合   \n",
    "    A是有限的动作集合  \n",
    "    P是基于环境的状态转移矩阵：其中每一项为Agent在某个状态s下，采取动作a后，与Environment交互后转移到其它状态s'的概率值，表示为P(s'|s,a) 。   \n",
    "    R是奖励函数：Agent在某个状态s下，采取动作a后，与Environment交互后所获得的奖励，表示为R(s,a)。   \n",
    "    $ \\gamma $(gamma)是折扣因子(discounted factor)，取值区间为[0,1]。   \n",
    " \n",
    "所以MDP过程可以表示为(S,A,P,R,gamma)。如果该过程中的状态转移矩阵P和奖励R(s,a)对于Agent都是可见的，我们称这样的RL Agent为Model-based Agent，否则称为Model-freee Agent。   \n",
    "RL Agent包含三个重要组件，分别为Model，Value function，Policy。其各自的作用为：\n",
    "    Policy: Agent的策略函数，输入当前状态s属于S，输出要采取的行动a属于A ;   \n",
    "    Value function: 评价Agent在某一个状态（或在某一状态采取某一行动）所能获得的期望奖励；  \n",
    "    Model: Agent对当前环境的建模如状态表示及其转移的建模和奖励建模；这里我们先讨论最基本的Model-based的MDP，即该MDP：(1) 状态之间的转移是已知可见的；(2) 不同状态采取不同动作所能获得的奖励是已知可见的。接下来依次介绍Policy和Value function。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy in MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value funtion for MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Expectation Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB2-Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T00:26:42.286833Z",
     "start_time": "2021-04-28T00:26:42.283512Z"
    }
   },
   "source": [
    "### ADPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T00:26:48.890229Z",
     "start_time": "2021-04-28T00:26:48.887071Z"
    }
   },
   "source": [
    "### TDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T00:26:43.484064Z",
     "start_time": "2021-04-28T00:26:43.481366Z"
    }
   },
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB3-Pytorch_Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB4-AgentMoving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.double-q  \n",
    "3.per  \n",
    "4.dueling  \n",
    "5.noisy-net  \n",
    "6.categorical-dqn  \n",
    "7.n_step_learning  \n",
    "8.raninbow  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB5-Multi Ageent Reinforcement Learning---- Basic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Joint Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minmax-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB6-MADDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Agent Deep Deterministic Policy Gradient，基于DDPG：深度确定性策略梯度算法的多智能体强化学习框架。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      模型由多个DDPG网络组成，每个网络学习policy π (Actor) 和 action value Q (Critic)；同时具有target network，用于Q-learning的off-policy学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./Assort/6-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T00:51:32.186754Z",
     "start_time": "2021-04-28T00:51:32.184833Z"
    }
   },
   "source": [
    "### 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./Assort/6-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 整体如图，采样收集数据即执行部分是分别进行的，训练学习是统一进行的。      \n",
    "2. 各个Actor收集数据(s, a=μ(s), r, s_next, a'=μ'(s))，并存入Replay Buffer，当缓存池数量大于预热阈值时，开始学习。        \n",
    "3. 每个Actor分别更新policy π参数，与DDPG一样，只需要当前(s, a=μ(s))。      \n",
    "4. 每个Critic分别更新action value Q参数，注意每个Critic都能看到所有的Actor收集的数据，更新参数时会考虑所有Actor生成的数据，即优化的是每个Critic对全局的贡献最大。    \n",
    "5. 重复2，3，4，直至收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特点：\n",
    "1. 集中式训练，分布式执行：训练时采用集中式学习训练critic与actor，使用时actor只用知道局部信息就能运行。critic需要其他智能体的策略信息，本文给了一种估计其他智能体策略的方法，能够只用知道其他智能体的观测与动作。\n",
    "2. 改进了经验回放记录的数据。为了能够适用于动态环境，每一条信息由 $ (x,x',a_q...,a_n,r_1,r_n) $ 组成，$ x=(o_1,...,o_n) $ 表示每个智能体的观测。\n",
    "3. 利用策略集合效果优化（policy ensemble）：对每个智能体学习多个策略，改进时利用所有策略的整体效果进行优化。以提高算法的稳定性以及鲁棒性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T01:02:59.198646Z",
     "start_time": "2021-04-28T01:02:59.195255Z"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
